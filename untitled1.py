# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ct64oyVCvB1d1o7saWCCOPh0FuywTu3e
"""

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset

df = pd.read_csv("hf://datasets/AashishKumar/Flirty_trainer/flirty_dataset.csv")
dataset = Dataset.from_pandas(df)

df.head()

dataset

"""**Load Model & Tokenizer**"""

from transformers import BartTokenizer, TFBartForConditionalGeneration

model_name = "facebook/bart-base"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = TFBartForConditionalGeneration.from_pretrained(
    model_name,
    from_pt=True
)

"""**Preprocess Data**"""

# Preprocess function
def preprocess_function(examples):
    inputs = [ex for ex in examples["input_hinglish"]]
    targets = [ex for ex in examples["output_hinglish"]]
    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding="max_length")

    labels = tokenizer(targets, max_length=64, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Map preprocessing
tokenized_datasets = dataset.map(preprocess_function, batched=True)

tokenized_datasets

"""**Prepare TensorFlow Dataset**"""

# Convert Dataset column to list
import tensorflow as tf
inputs_texts = tokenized_datasets["input_hinglish"][:]
outputs_texts = tokenized_datasets["output_hinglish"][:]

# Now tokenize
inputs = tokenizer(list(inputs_texts), padding=True, truncation=True, return_tensors="tf")
labels = tokenizer(list(outputs_texts), padding=True, truncation=True, return_tensors="tf")

# Prepare TF dataset dict
tokenized_dataset_tf = {
    "input_ids": inputs["input_ids"],
    "attention_mask": inputs["attention_mask"],
    "labels": labels["input_ids"]
}

# Convert to tf.data.Dataset
def convert_to_tf_dataset(tokenized_dataset, batch_size=8):
    dataset = tf.data.Dataset.from_tensor_slices((
        {"input_ids": tokenized_dataset["input_ids"], "attention_mask": tokenized_dataset["attention_mask"]},
        tokenized_dataset["labels"]
    ))
    dataset = dataset.shuffle(1000).batch(batch_size)
    return dataset

train_dataset = convert_to_tf_dataset(tokenized_dataset_tf, batch_size=8)
eval_dataset = train_dataset  # small dataset

train_dataset

"""**Compile & Train Model**"""

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)

model.compile(optimizer=optimizer)

model.fit(train_dataset, epochs=10)

model.save_pretrained("./hinglish_model_tf")
tokenizer.save_pretrained("./hinglish_model_tf")

from transformers import BartTokenizer, TFBartForConditionalGeneration

# Load tokenizer and model from the folder where you saved fine-tuned TF model
tokenizer = BartTokenizer.from_pretrained("./hinglish_model_tf")
model = TFBartForConditionalGeneration.from_pretrained("./hinglish_model_tf")

def chat(inp):
    # Wrap in list to make batch
    inputs = tokenizer([inp], return_tensors="tf", padding=True, truncation=True)

    # Generate
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50,
        do_sample=True,
        top_k=50,
        top_p=0.9
    )

    # Decode first batch element
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

chat("Kya tumhe chai pasand hai?")